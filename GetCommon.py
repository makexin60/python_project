import json
import logging
import os
import re
import time
from collections import OrderedDict
from urllib.parse import urlparse, urlunparse

from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.common import TimeoutException
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.action_chains import ActionChains
from selenium.webdriver.common.by import By
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.support.wait import WebDriverWait
from webdriver_manager.chrome import ChromeDriverManager

limit_time = 0.8

def getLogger(address):
    # åˆ›å»ºæ—¥å¿—è®°å½•å™¨
    logger = logging.getLogger("my_logger")
    logger.setLevel(logging.DEBUG)  # å…è®¸æ‰€æœ‰çº§åˆ«çš„æ—¥å¿—è¿›å…¥å¤„ç†å™¨

    # å®šä¹‰æ—¥å¿—æ ¼å¼ï¼ˆåŒ…å«æ¯«ç§’ï¼‰
    log_format = logging.Formatter(
        "%(asctime)s,%(msecs)03d - %(levelname)s - %(message)s",
        datefmt="%Y-%m-%d %H:%M:%S"
    )

    # åˆ›å»º ERROR çº§åˆ«æ—¥å¿—æ–‡ä»¶å¤„ç†å™¨
    error_handler = logging.FileHandler(f"Logger/{address}_error.log",encoding="utf-8")
    error_handler.setLevel(logging.ERROR)
    error_handler.setFormatter(log_format)

    # åˆ›å»º WARNING çº§åˆ«æ—¥å¿—æ–‡ä»¶å¤„ç†å™¨
    warning_handler = logging.FileHandler(f"Logger/{address}_warning.log",encoding="utf-8")
    warning_handler.setLevel(logging.WARNING)
    warning_handler.setFormatter(log_format)

    # æ·»åŠ è¿‡æ»¤å™¨ï¼Œç¡®ä¿ warning_handler åªå¤„ç† WARNING çº§åˆ«æ—¥å¿—
    warning_handler.addFilter(lambda record: record.levelno == logging.WARNING)

    # æ·»åŠ è¿‡æ»¤å™¨ï¼Œç¡®ä¿ error_handler åªå¤„ç† ERROR åŠä»¥ä¸Šæ—¥å¿—
    error_handler.addFilter(lambda record: record.levelno >= logging.ERROR)

    # æ·»åŠ å¤„ç†å™¨åˆ°æ—¥å¿—è®°å½•å™¨
    logger.addHandler(error_handler)
    logger.addHandler(warning_handler)

    return logger

def get_app_info(target_path):
    with open(target_path, "r", encoding="utf-8") as file:
        content = json.load(file, object_pairs_hook=lambda pairs: OrderedDict(pairs))
    print(f"open {target_path} file length: {len(content)}")
    return content

def save_new_txt_file(target_path,data):
    with open(target_path, "w", encoding="utf-8") as f:
        f.write(data)

def save_new_json_file(target_path, data):
    with open(target_path, "w", encoding="utf-8") as file:
        json.dump(data, file, ensure_ascii=False, indent=4)

def save_add_json_file(target_path, data):
    with open(target_path, "a+", encoding="utf-8") as f:
        f.write(json.dumps(data, ensure_ascii=False, indent=4) + ",\n")


def scroll_to_load_apps(driver, max_scrolls=10):
    """æ»šåŠ¨åˆ°åº•éƒ¨åŠ è½½æ›´å¤šåº”ç”¨"""
    print("ğŸ”½ å¼€å§‹æ»šåŠ¨ä»¥åŠ è½½æ›´å¤šåº”ç”¨...")

    last_height = driver.execute_script("return document.body.scrollHeight")
    scroll_attempt = 0

    while scroll_attempt < max_scrolls:
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")

        try:
            # **ç­‰å¾…æ–°å†…å®¹åŠ è½½**ï¼ˆæœ€å¤š 5 ç§’ï¼‰
            WebDriverWait(driver, 2).until(
                lambda d: driver.execute_script("return document.body.scrollHeight") > last_height
            )
        except Exception:
            print("âš ï¸ æœªæ£€æµ‹åˆ°æ–°å†…å®¹ï¼Œå¯èƒ½å·²åŠ è½½å®Œæ‰€æœ‰åº”ç”¨")

        new_height = driver.execute_script("return document.body.scrollHeight")

        if new_height == last_height:  # æ²¡æœ‰æ–°å†…å®¹äº†ï¼Œåœæ­¢æ»šåŠ¨
            print("âœ… æ»šåŠ¨ç»“æŸï¼Œæ‰€æœ‰åº”ç”¨å·²åŠ è½½")
            break

        last_height = new_height
        scroll_attempt += 1

    print("ğŸ”¼ ç»“æŸæ»šåŠ¨")

# def scroll_to_load_apps(driver):
#     """æ»šåŠ¨åˆ°åº•éƒ¨åŠ è½½æ›´å¤šåº”ç”¨"""
#     last_height = driver.execute_script("return document.body.scrollHeight")
#
#     while True:
#         driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
#         time.sleep(limit_time)  # ç­‰å¾…åŠ è½½
#
#         new_height = driver.execute_script("return document.body.scrollHeight")
#         if new_height == last_height:  # æ²¡æœ‰æ–°å†…å®¹äº†ï¼Œåœæ­¢æ»šåŠ¨
#             print("leave scroll_to_load_apps")
#             break
#         last_height = new_height

def extract_app_ids(driver):
    """æå–å½“å‰é¡µé¢çš„æ‰€æœ‰ app_id"""
    app_ids = set()
    app_links = driver.find_elements(By.XPATH, "//a[contains(@href, '/store/apps/details?id=')]")
    for link in app_links:
        match = re.search(r"id=([^&]+)", link.get_attribute("href"))
        if match:
            app_ids.add(match.group(1))
    return app_ids

def scrape_category_no_click(url,driver):
    print(f"enter scrape_category_no_click")
    driver.set_page_load_timeout(15)
    driver.get(url)

    try:
        # **ç­‰å¾…æ–°é¡µé¢åŠ è½½å®Œæˆ**
        WebDriverWait(driver, 10).until(
            EC.presence_of_element_located((By.TAG_NAME, "body"))
        )
    except Exception:
        print("âš ï¸ é¡µé¢ä¸»è¦å…ƒç´ æœªæ‰¾åˆ°ï¼Œå¯èƒ½éœ€è¦è°ƒæ•´ XPATHï¼")
        return None

    scroll_to_load_apps(driver)  # åªæ»šåŠ¨åˆ°åº•éƒ¨ï¼Œä¸ç‚¹å‡»æŒ‰é’®
    app_ids = extract_app_ids(driver)

    print(f"leave {url} get {len(app_ids)} applications")
    return app_ids

def scrape_category_no_click_no_url(driver):
    time.sleep(limit_time)
    scroll_to_load_apps(driver)  # åªæ»šåŠ¨åˆ°åº•éƒ¨ï¼Œä¸ç‚¹å‡»æŒ‰é’®
    app_ids = extract_app_ids(driver)

    print(f"leave scrape_category_no_click_no_url get {len(app_ids)} applications")
    return app_ids

def scroll_and_click_show_more(driver):
    print("enter scroll_and_click_show_more")
    while True:
        # å…ˆæ»šåŠ¨åˆ°åº•éƒ¨ï¼Œç¡®ä¿æ‰€æœ‰å†…å®¹åŠ è½½å‡ºæ¥
        scroll_to_load_apps(driver)
        time.sleep(limit_time)
        # å°è¯•ç‚¹å‡» 'Show more' æŒ‰é’®
        try:
            show_more_button = driver.find_element(By.XPATH, "//span[contains(text(), 'Show More')]")
            ActionChains(driver).move_to_element(show_more_button).click().perform()
            print("âœ… click 'Show more' button")
            time.sleep(limit_time)  # ç­‰å¾…æ–°å†…å®¹åŠ è½½

            # **ç‚¹å‡»æŒ‰é’®åï¼Œå†æ¬¡æ»šåŠ¨åˆ°åº•éƒ¨ï¼Œç¡®ä¿æ–°å†…å®¹åŠ è½½**
            scroll_to_load_apps(driver)

        except Exception as e:
            print(f"leave scroll_and_click_show_more  âŒ not find 'Show more' button{e}")
            break

        # **æ£€æŸ¥é¡µé¢æ˜¯å¦ç»§ç»­å¢é•¿**
        last_height = driver.execute_script("return document.documentElement.scrollHeight")
        time.sleep(limit_time)
        new_height = driver.execute_script("return document.documentElement.scrollHeight")

        if new_height == last_height:
            print("leave scroll_and_click_show_more")
            break

#scroll page and click button
def scrape_category_click(url,driver):
    driver.get(url)
    time.sleep(limit_time)

    scroll_and_click_show_more(driver)
    app_ids = extract_app_ids(driver)

    print(f"leave scrape_category_click {url} get {len(app_ids)} applications")
    return app_ids

#remove_duplicates_json in two files
def remove_duplicates_json(file1,target_file,output_file):
    try:
        with open(file1, "r", encoding="utf-8") as f1:
            data1 = set(json.load(f1))  # è§£æ JSONï¼Œè½¬æ¢ä¸ºé›†åˆ
        with open(target_file, "r", encoding="utf-8") as f2:
            data2 = set(json.load(f2))
    except json.JSONDecodeError as e:
        print(f"âŒ JSON parse error: {e}")
        return
    except FileNotFoundError as e:
        print(f"âŒ file not find: {e}")
        return

    # è®¡ç®—å·®é›†ï¼ˆåªä¿ç•™ file2 ä¸­ä¸åœ¨ file1 é‡Œçš„å†…å®¹ï¼‰
    new_app_ids = list(data2 - data1)

    # å†™å…¥æ–°çš„ JSON æ–‡ä»¶
    save_new_json_file(output_file,new_app_ids)

    print(f"âœ… successï¼Œdata1:{len(data1)}, data2:{len(data2)}, new_app_ids:{len(new_app_ids)}, resule file path{output_file}")

# åªä¸‹è½½ chromedriver ä¸€æ¬¡ï¼Œé¿å…é‡å¤å®‰è£…
CHROMEDRIVER_PATH = ChromeDriverManager().install()

def setup_driver():
    options = Options()
    options.add_argument("--headless")
    options.add_argument("--disable-gpu")
    options.add_argument("--disable-extensions")
    options.add_argument("--disable-infobars")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument("--window-size=1920,1080")
    options.add_argument("--no-sandbox")
    options.add_argument("--lang=en-US")
    options.add_argument("--disable-blink-features=AutomationControlled")
    options.add_argument(
        "user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0 Safari/537.36"
    )
    prefs = {"profile.managed_default_content_settings.images": 2,
             "profile.managed_default_content_settings.javascript": 1}
    options.add_experimental_option("prefs", prefs)
    service = Service(CHROMEDRIVER_PATH)
    driver = webdriver.Chrome(service=service, options=options)

    return driver

def wait_span_highlights(click_path,driver):
    print("enter wait_span_highlights")

    try:
        # ç­‰å¾…æ‰€æœ‰åŒ…å« "Highlights" æ–‡å­—çš„ <span> å‡ºç°
        element = WebDriverWait(driver, 10).until(
            EC.presence_of_element_located((By.XPATH, click_path))
        )

        if element:
            print("Found 'Highlights', performing actions...")

            # æŸ¥æ‰¾æ‰€æœ‰ <span> æ ‡ç­¾ï¼Œæ–‡æœ¬å†…å®¹ä¸º "Highlights"
            highlights_spans = driver.find_elements(By.XPATH, click_path)

            # ä¾æ¬¡ç‚¹å‡»
            for span in highlights_spans:
                # ç¡®ä¿å…ƒç´ å¯è§å¹¶å¯ç‚¹å‡»
                driver.execute_script("arguments[0].scrollIntoView();", span)  # æ»šåŠ¨åˆ°å…ƒç´ 
                WebDriverWait(driver, 10).until(EC.element_to_be_clickable(span)).click()

    except TimeoutException:
        print("TimeoutException wait_span_highlights:")

    except Exception as e:
        # **å…¶ä»–å¼‚å¸¸æ‰“å°é”™è¯¯å¹¶æŠ›å‡º**
        print("Error wait_span_highlights:", e)

    print("leave wait_span_highlights")

def get_privacy_original(page_source):
    print(f"enter get_privacy_original")
    soup = BeautifulSoup(page_source, "html.parser")

    # **å®šä¹‰å¯èƒ½çš„å†…å®¹åŒºåŸŸ**
    possible_selectors = [
        "main#brx-content",  # Justalk
        "div[class*='xg6iff7']",  # Facebook å³åŠéƒ¨åˆ†
        "article",
        "main",
        "body"  # å…œåº•ç­–ç•¥
    ]

    privacy_text = ""

    for selector in possible_selectors:
        content = soup.select_one(selector)
        if content:
            # **åˆ é™¤æ‰€æœ‰ <header> å’Œ <footer> æ ‡ç­¾**
            for tag in content.find_all(["header", "footer"]):
                tag.decompose()

            # **åˆ é™¤æ‰€æœ‰ class åŒ…å« 'header' æˆ– 'footer' çš„å…ƒç´ **
            for tag in content.find_all(class_=lambda c: c and ("header" in c.lower() or "footer" in c.lower())):
                tag.decompose()

            time.sleep(limit_time)
            # **è·å–æ¸…ç†åçš„æ–‡æœ¬**
            privacy_text = content.get_text(separator="\n", strip=True)
            print("okay get_privacy_original")
            return privacy_text

    print(f"cant find content get_privacy_original")
    return privacy_text

def get_visible_privacy_links(driver):
    script = """
    return Array.from(document.querySelectorAll('a')).filter(a => {
        let rect = a.getBoundingClientRect();
        if (!a.textContent.toLowerCase().includes('privacy')) return false;
        if (document.head.contains(a)) return false;  // ğŸš« <head> é‡Œé¢çš„ <a> ä¸è¦

        let elem = a;
        while (elem) {
            let style = window.getComputedStyle(elem);
            if (style.display === 'none' || style.visibility === 'hidden' || style.opacity === '0') {
                return false;  // ğŸš« ç¥–å…ˆéšè—
            }
            if (elem.tagName.toLowerCase() === 'footer' || elem.tagName.toLowerCase() === 'header') {
                return false;  // ğŸš« <footer> å’Œ <header> é‡Œçš„ <a> ä¸è¦
            }
            elem = elem.parentElement;
        }

        return true;  // âœ… ç¬¦åˆæ¡ä»¶çš„ <a> 
    }).map(a => a.href);
    """
    return driver.execute_script(script)

def filter_urls(urls,original_url):
    print(f"enter filter_urls original_urls:{urls}")

    seen_paths = set()  # è®°å½•å·²ç»å‡ºç°è¿‡çš„è·¯å¾„ï¼ˆå¿½ç•¥è¯­è¨€/åœ°åŒºä»£ç ã€fragment å’Œç»“å°¾ `/`ï¼‰
    seen_urls = set()  # è®°å½•å·²å‡ºç°è¿‡çš„å®Œæ•´ URL

    normalized_url = normalize_url(original_url)
    seen_paths.add(normalized_url)

    filtered_urls = []

    for url in urls:
        # **ç›´æ¥ä¸¢å¼ƒ `mailto:` URL**
        if url.startswith("mailto:"):
            continue

        # **å»æ‰ä¸æ˜¯ http æˆ– https å¼€å¤´çš„ URL**
        if not (url.startswith("http://") or url.startswith("https://")):
            continue  # ä¸¢å¼ƒè¯¥ URL

        # **æ ‡å‡†åŒ– URL**
        normalized_url = normalize_url(url)

        # **æ£€æŸ¥ `seen_paths`ï¼ˆæ˜¯å¦å·²ç»è®¿é—®è¿‡ç›¸ä¼¼çš„è·¯å¾„ï¼‰**
        if normalized_url in seen_paths:
            continue  # ä¸¢å¼ƒ

        seen_paths.add(normalized_url)  # è®°å½•è·¯å¾„ï¼ˆå¿½ç•¥ fragment å’Œ `/`ï¼‰

        # **æ£€æŸ¥ `seen_urls`ï¼ˆå®Œæ•´ URL å»é‡ï¼‰**
        if url in seen_urls:
            continue  # ä¸¢å¼ƒ

        seen_urls.add(url)  # è®°å½•å®Œæ•´ URL

        # **ä¿ç•™ URL**
        filtered_urls.append(url)

    print(f"leave filter_urls: {filtered_urls}")
    return filtered_urls

def normalize_url(url):
    parsed = urlparse(url)

    # **å»æ‰ fragment**
    parsed = parsed._replace(fragment="")

    # **å»æ‰ç»“å°¾ `/`**
    path = parsed.path.rstrip("/")

    # **å¿½ç•¥ www.**
    netloc = parsed.netloc.lower().replace("www.", "")

    # **å¿½ç•¥ http å’Œ https**
    scheme = "https"  # ç»Ÿä¸€æˆ https

    # **é‡æ–°æ„é€  URL**
    normalized = urlunparse((scheme, netloc, path, "", "", ""))

    return normalized

def read_id(original_path,target_path,express):
    with open(original_path, "r", encoding="utf-8") as file:
        for line in file:
            json_str = line.strip()
            match = re.search(express, json_str)
            if match:
                app_id = match.group(1)
                save_add_json_file(target_path, app_id)
                print(f"ğŸ“Œ extract App ID: {app_id}")
            else:
                print("âŒ cant find App ID")

    print(f"save file path: {target_path}")

def delete_matching_files(directory, id_file):
    # è¯»å– JSON æ–‡ä»¶ï¼Œè·å–è¦åˆ é™¤çš„æ–‡ä»¶ååˆ—è¡¨
    with open(id_file, "r", encoding="utf-8") as f:
        id_list = json.load(f)  # è¯»å– ["1477079218", "1203185844", ...]

    # éå†ç›®æ ‡ç›®å½•ä¸‹çš„æ‰€æœ‰æ–‡ä»¶
    for filename in os.listdir(directory):
        file_path = os.path.join(directory, filename)
        # å»æ‰æ–‡ä»¶æ‰©å±•å
        file_name_without_ext, _ = os.path.splitext(filename)
        # å»æ‰æœ€åä¸€ä¸ª "_" åŠå…¶åé¢çš„å†…å®¹
        new_name = file_name_without_ext.rsplit("_", 1)[0]  # ä¸åŠ æ‰©å±•å

        # å¦‚æœæ–‡ä»¶ååœ¨ ID åˆ—è¡¨ä¸­ï¼Œåˆ™åˆ é™¤
        if new_name in id_list:
            try:
                os.remove(file_path)
                print(f"âœ… å·²åˆ é™¤: {file_path}")
            except Exception as e:
                print(f"âŒ åˆ é™¤å¤±è´¥: {file_path}, é”™è¯¯: {e}")

def wait_extra_privacy_only_sub(results,filtered_urls,driver,logger):
    print(f"enter wait_extra_privacy")

    sub_privacy_data = "Sub-section Policy: "
    results.update({
        "sub-section_urls": filtered_urls
    })

    for index, href in enumerate(filtered_urls):
        try:
            driver.set_page_load_timeout(10)  # æœ€å¤šç­‰å¾… 10 ç§’
            # **è·³è½¬åˆ°è¯¥é“¾æ¥**
            driver.get(href)
            driver.execute_script("window.stop();")  # Stop unnecessary loading

            # **ç­‰å¾…æ–°é¡µé¢åŠ è½½å®Œæˆ**
            WebDriverWait(driver, 10).until(
                EC.presence_of_element_located((By.TAG_NAME, "body"))
            )

            # **è·å–æ–°é¡µé¢çš„æ•°æ®**
            new_page_content = driver.page_source

            soup = BeautifulSoup(new_page_content, "html.parser")

            # è·å– id="ph-body" çš„ div
            body_content = soup.find(id="ph-body")

            # å¦‚æœæ‰¾ä¸åˆ°ï¼Œåˆ™å›é€€åˆ° body
            if not body_content:
                body_content = soup.select_one("body")

            if body_content:
                # **åˆ é™¤ header å’Œ footer ä»¥åŠ class åç§°åŒ…å« 'header' æˆ– 'footer' çš„å…ƒç´ **
                for tag in body_content.find_all(["header", "footer"]):  # åˆ é™¤ <header> å’Œ <footer> ç›´æ¥æ ‡ç­¾
                    tag.decompose()

                for tag in body_content.find_all(
                        class_=lambda c: c and ("header" in c.lower() or "footer" in c.lower())):
                    tag.decompose()  # åˆ é™¤ class åŒ…å« 'header' æˆ– 'footer' çš„å…ƒç´ 

                time.sleep(limit_time)
                sub_data = body_content.get_text(separator="\n", strip=True)
                sub_privacy_data += "\n" + "Sub-policy " + str(index) + "-" + href + ":\n" + sub_data  # æ¯æ¬¡æ‹¼æ¥ååŠ çœŸå®æ¢è¡Œç¬¦

        except TimeoutException:
            sub_privacy_data += "\n" + "Sub-policy " + str(index) + "-" + href + ":\n"
            logger.error(f"wait_extra_privacy TimeoutException: {(json.dumps(results, ensure_ascii=False))}")
            continue

        except Exception:
            sub_privacy_data += "\n" + "Sub-policy " + str(index) + "-" + href + ":\n"
            logger.error(f"Error more links:{href}---{(json.dumps(results, ensure_ascii=False))}")
            continue

    print("leave wait_extra_privacy")
    return sub_privacy_data,results

def wait_extra_privacy(results, privacy_data,filtered_urls,driver,logger):
    print(f"enter wait_extra_privacy")

    privacy_data += privacy_data + "\n" + "Sub-section Policy: "
    results.update({
        "sub-section_urls": filtered_urls
    })

    for index, href in enumerate(filtered_urls):
        try:
            driver.set_page_load_timeout(10)  # æœ€å¤šç­‰å¾… 10 ç§’
            # **è·³è½¬åˆ°è¯¥é“¾æ¥**
            driver.get(href)

            # **ç­‰å¾…æ–°é¡µé¢åŠ è½½å®Œæˆ**
            WebDriverWait(driver, 10).until(
                EC.presence_of_element_located((By.TAG_NAME, "body"))
            )

            # **è·å–æ–°é¡µé¢çš„æ•°æ®**
            new_page_content = driver.page_source

            soup = BeautifulSoup(new_page_content, "html.parser")

            # è·å– id="ph-body" çš„ div
            body_content = soup.find(id="ph-body")

            # å¦‚æœæ‰¾ä¸åˆ°ï¼Œåˆ™å›é€€åˆ° body
            if not body_content:
                body_content = soup.select_one("body")

            if body_content:
                # **åˆ é™¤ header å’Œ footer ä»¥åŠ class åç§°åŒ…å« 'header' æˆ– 'footer' çš„å…ƒç´ **
                for tag in body_content.find_all(["header", "footer"]):  # åˆ é™¤ <header> å’Œ <footer> ç›´æ¥æ ‡ç­¾
                    tag.decompose()

                for tag in body_content.find_all(
                        class_=lambda c: c and ("header" in c.lower() or "footer" in c.lower())):
                    tag.decompose()  # åˆ é™¤ class åŒ…å« 'header' æˆ– 'footer' çš„å…ƒç´ 

                time.sleep(limit_time)
                privacy_text = body_content.get_text(separator="\n", strip=True)
                privacy_data += "\n" + "Sub-policy " + str(index) + "\n" + href + ":\n" + privacy_text  # æ¯æ¬¡æ‹¼æ¥ååŠ çœŸå®æ¢è¡Œç¬¦

        except TimeoutException:
            privacy_data += "\n" + "Sub-policy " + str(index) + "\n" + href + ":\n"
            logger.error(f"wait_extra_privacy TimeoutException: {(json.dumps(results, ensure_ascii=False))}")
            continue

        except Exception:
            privacy_data += "\n" + "Sub-policy " + str(index) + "\n" + href + ":\n"
            logger.error(f"Error more links:{href}---{(json.dumps(results, ensure_ascii=False))}")
            continue


    print("leave wait_extra_privacy")
    return privacy_data,results

# calculate content file2 have but file1 not have
def filter_content(file1,file2,output_file):

    # è¯»å– A.json
    with open(file1, "r", encoding="utf-8") as file_A:
        original = set(json.load(file_A))  # è½¬æ¢ä¸ºé›†åˆï¼Œæ–¹ä¾¿å»é‡å¯¹æ¯”

    # è¯»å– A_bac.json
    with open(file2, "r", encoding="utf-8") as file_A_bac:
        bac = set(json.load(file_A_bac))

    diff = list(bac - original)

    # å†™å…¥æœ€ç»ˆæ–‡ä»¶
    with open(output_file, "w", encoding="utf-8") as output:
        json.dump(diff, output, ensure_ascii=False, indent=4)

    print("å¤„ç†å®Œæˆï¼Œç”Ÿæˆæ–‡ä»¶ï¼š", output_file)

def convert_upper(path,output_path):
    data = get_app_info(path)
    data = [str(item).upper() for item in data]
    save_new_json_file(output_path,data)

def get_file_name(directory,target_path,size,flag,non_200_app_id_path):
    files = [
        f for f in os.listdir(directory)
        if os.path.isfile(os.path.join(directory, f)) and os.path.getsize(os.path.join(directory, f)) < size
    ]
    print(f"Number of files in '{directory}': {len(files)}")

    if flag == "size":
        # extract the content before the last `_`
        for file in files:
            file_prefix = file.rsplit("_", 1)[0]
            save_add_json_file(target_path, file_prefix)
    else:
        error_codes = {"301", "302", "303", "307", "400", "403", "404", "408",  "410", "429", "500", "502", "503", "504"}

        for file in files:
            file_path = os.path.join(directory, file)
            file_prefix = file.rsplit("_", 1)[0]

            try:
                with open(file_path, "r", encoding="utf-8") as f:
                    lines = f.readlines()

                if len(lines) < 4:
                    print(lines)
                    file_content = " ".join(lines)
                    if any(code in file_content for code in error_codes):
                        save_add_json_file(non_200_app_id_path, file_prefix)
                    else:
                        save_add_json_file(target_path, file_prefix)
            except Exception as e:
                print(f"âŒ handel {file_path} fail: {e}")

def filter_data(file_a, file_b, output_file):
    # è¯»å– A æ–‡ä»¶ï¼ˆåˆ—è¡¨ï¼‰
    with open(file_a, "r", encoding="utf-8") as f:
        id_list = json.load(f)  # A æ–‡ä»¶å­˜çš„æ˜¯ ["1000385101", "1000551625", ...]

    # è¯»å– B æ–‡ä»¶ï¼ˆåŒ…å«å­—å…¸çš„åˆ—è¡¨ï¼‰
    with open(file_b, "r", encoding="utf-8") as f:
        data_list = json.load(f)  # B æ–‡ä»¶å­˜çš„æ˜¯ [{"id": 1214208722, "name": "...", "url": "..."}, ...]

    # è¿‡æ»¤ B æ–‡ä»¶ä¸­çš„æ•°æ®
    filtered_data = [item for item in data_list if str(item["id"]) in id_list]

    # å°†åŒ¹é…çš„ç»“æœå†™å…¥æ–°æ–‡ä»¶
    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(filtered_data, f, indent=4, ensure_ascii=False)

    print(f"âœ… ç»“æœå·²ä¿å­˜åˆ° {output_file}")


def get_header():
    authorization = "Bearer eyJhbGciOiJFUzI1NiIsInR5cCI6IkpXVCIsImtpZCI6IlU4UlRZVjVaRFMifQ.eyJpc3MiOiI3TktaMlZQNDhaIiwiaWF0IjoxNzM5MTkyNzIzLCJleHAiOjE3NDY0NTAzMjMsInJvb3RfaHR0cHNfb3JpZ2luIjpbImFwcGxlLmNvbSJdfQ.wuLUSBq_qWD4MAkmwbxoaJDQTRyUBZzEqDMuvT-nAJNarxda8F5TLXpRy_OjonWBuUcnD-eyWb86qn4w6xfang"

    # Headers (Apple may require authorization tokens, these are standard ones)
    headers = {
        "User-Agent": "Mozilla/5.0",
        "Accept": "*/*",
        "Origin": "https://apps.apple.com",
        "Referer": "https://apps.apple.com/",
        "Authorization": authorization
    }

    return headers